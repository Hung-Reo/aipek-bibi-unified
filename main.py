# main.py - Unified FastAPI server for BiBi (Backend + Frontend)
# ‚úÖ STRATEGY: Keep 100% of proven backend logic + Add static serving

import sys
import os
import json
import traceback
import uvicorn
from fastapi import FastAPI, Query, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from typing import List, Optional, Dict, Any
from fastapi.responses import StreamingResponse
import openai
from pydantic import BaseModel, Field
from routes.tts import router as tts_router

# Th√™m th∆∞ m·ª•c g·ªëc v√†o sys.path ƒë·ªÉ import ƒë∆∞·ª£c c√°c module
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '.')))

# ƒê·ªãnh nghƒ©a c√°c namespace m·∫∑c ƒë·ªãnh
BIBI_NAMESPACE = "bibi_grammar"
AVAILABLE_NAMESPACES = ["bibi_grammar", "bibi_sgk", "bibi_ctgd", "bibi_others"]

# Import retriever_service t·ª´ ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c
try:
    from app.core.retriever import retriever_service
    from app.config import BIBI_PINECONE_INDEX

    print(f"‚úÖ ƒê√£ k·∫øt n·ªëi th√†nh c√¥ng v·ªõi d·ªãch v·ª• RAG - Pinecone index: {BIBI_PINECONE_INDEX}")
except ImportError as e:
    print(f"‚ùå L·ªói import retriever_service: {str(e)}")
    raise

# Model cho request t·ª´ frontend
class ChatRequest(BaseModel):
    messages: List[Dict[str, Any]]
    model: str = "gpt-4.1"
    temperature: float = 0.7
    max_tokens: int = 2000
    stream: bool = True

# Ki·ªÉm tra phi√™n b·∫£n OpenAI
try:
    OPENAI_VERSION = int(openai.__version__.split('.')[0])
    print(f"üìå ƒê√£ ph√°t hi·ªán OpenAI API phi√™n b·∫£n: {openai.__version__} (Ch√≠nh: {OPENAI_VERSION})")
except (ImportError, AttributeError, ValueError):
    OPENAI_VERSION = 0
    print("‚ö†Ô∏è Kh√¥ng th·ªÉ x√°c ƒë·ªãnh phi√™n b·∫£n OpenAI API, gi·∫£ ƒë·ªãnh phi√™n b·∫£n c≈©")

# ‚úÖ UNIFIED: T·∫°o FastAPI app v·ªõi static serving
app = FastAPI(
    title="BiBi Unified API",
    version="2.0.0",
    description="Unified API cho BiBi - Tr·ª£ l√Ω AI Gi√°o d·ª•c K12 (Backend + Frontend)"
)

# ‚úÖ UNIFIED: Mount static files directory
app.mount("/static", StaticFiles(directory="static"), name="static")

# ‚úÖ UNIFIED: Setup templates directory
templates = Jinja2Templates(directory="templates")

# C·∫•u h√¨nh CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["Content-Length"],
    max_age=600,
)

# Mount TTS routes
app.include_router(tts_router, prefix="/api/tts", tags=["tts"])

# ===============================
# ‚úÖ STATIC FILE SERVING ROUTES
# ===============================

@app.get("/", response_class=HTMLResponse)
async def serve_home(request: Request):
    """Serve homepage with chatbot selection"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/index.html", response_class=HTMLResponse)
async def serve_index(request: Request):
    """Serve index page"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/bots/bibi_lesson_plan.html", response_class=HTMLResponse)
async def serve_lesson_plan(request: Request):
    """Serve lesson plan page"""
    return templates.TemplateResponse("bibi_lesson_plan.html", {"request": request})

@app.get("/bots/bibi_grammar.html", response_class=HTMLResponse)
async def serve_grammar_bot(request: Request):
    """Serve grammar bot page"""
    return templates.TemplateResponse("bibi_grammar.html", {"request": request})

@app.get("/bots/bibi_feedback_admin.html", response_class=HTMLResponse)
async def serve_feedback_admin(request: Request):
    """Serve feedback admin page"""
    return templates.TemplateResponse("bibi_feedback_admin.html", {"request": request})

# ===============================
# ‚úÖ API ENDPOINTS (100% PRESERVED)
# ===============================

@app.get("/api/health")
async def health_check():
    """Ki·ªÉm tra tr·∫°ng th√°i API v√† k·∫øt n·ªëi Pinecone"""
    try:
        # Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn Pinecone nh∆∞ng kh√¥ng tr·∫£ v·ªÅ object ph·ª©c t·∫°p
        try:
            vector_count = retriever_service.get_vector_count()
            is_connected = True
        except Exception as e:
            vector_count = 0
            is_connected = False

        return {
            "status": "ok",
            "message": "BiBi RAG API ƒëang ho·∫°t ƒë·ªông",
            "version": "2.0.0",
            "mode": "unified",
            "pinecone_index": BIBI_PINECONE_INDEX,
            "default_namespace": BIBI_NAMESPACE,
            "pinecone_stats": {
                "is_connected": is_connected,
                "vector_count": vector_count,
                "namespaces": AVAILABLE_NAMESPACES
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"L·ªói k·∫øt n·ªëi Pinecone: {str(e)}"
        }

@app.get("/api/rag")
async def search_documents(
    query: str = Query(..., description="Search query"),
    namespace: str = Query(BIBI_NAMESPACE, description="Namespace to search in"),
    top_k: int = Query(5, description="Number of results to return"),
    search_all: bool = Query(False, description="Search all namespaces")
):
    """T√¨m ki·∫øm t√†i li·ªáu v·ªõi truy v·∫•n"""
    try:
        print(f"üîç API Search: query='{query}', namespace={namespace}, top_k={top_k}")
        
        if search_all:
            print(f"üîÑ T√¨m ki·∫øm tr√™n t·∫•t c·∫£ namespace: {AVAILABLE_NAMESPACES}")
            docs = retriever_service.search_multiple_namespaces(
                query,
                namespaces=AVAILABLE_NAMESPACES,
                top_k=top_k
            )
        else:
            print(f"üîÑ T√¨m ki·∫øm trong namespace: {namespace}")
            docs = retriever_service.search(query, top_k=top_k, namespace=namespace)

        # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£, th·ª≠ t√¨m trong t·∫•t c·∫£ c√°c namespace
        if not docs and not search_all:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ trong namespace {namespace}. Th·ª≠ t√¨m trong t·∫•t c·∫£ namespace.")
            docs = retriever_service.search_multiple_namespaces(
                query,
                namespaces=AVAILABLE_NAMESPACES,
                top_k=top_k
            )

        # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£
        results = []
        for doc in docs:
            results.append({
                "content": doc.page_content,
                "metadata": doc.metadata
            })

        print(f"‚úÖ T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ cho '{query}'")

        return JSONResponse(
            content={
                "status": "success",
                "query": query,
                "namespace": namespace,
                "results": results
            },
            headers={"Content-Type": "application/json; charset=utf-8"}
        )

    except Exception as e:
        error_trace = traceback.format_exc()
        print(f"‚ùå L·ªói: {str(e)}")
        print(error_trace)

        return {
            "status": "error",
            "message": str(e),
            "traceback": error_trace
        }

@app.post("/api/chat")
async def chat_stream(request: Request):
    """
    API endpoint x·ª≠ l√Ω streaming response t·ª´ OpenAI
    H·ªó tr·ª£ c·∫£ hai phi√™n b·∫£n API (c≈© v√† m·ªõi)
    """
    try:
        # ƒê·ªçc d·ªØ li·ªáu t·ª´ request
        body = await request.json()
        messages = body.get("messages", [])
        model = body.get("model", "gpt-4")
        temperature = body.get("temperature", 0.7)
        max_tokens = body.get("max_tokens", 2000)

        print(f"üì® Nh·∫≠n y√™u c·∫ßu API chat v·ªõi: model={model}, messages={len(messages)}")

        # ƒê·∫£m b·∫£o API key ƒë∆∞·ª£c thi·∫øt l·∫≠p
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            print("‚ö†Ô∏è OPENAI_API_KEY kh√¥ng ƒë∆∞·ª£c thi·∫øt l·∫≠p trong bi·∫øn m√¥i tr∆∞·ªùng")
            raise ValueError("API key kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh")

        def stream_generator():
            try:
                print(f"üîÑ S·ª≠ d·ª•ng OpenAI API phi√™n b·∫£n: {'m·ªõi (v1)' if OPENAI_VERSION >= 1 else 'c≈©'}")

                if OPENAI_VERSION >= 1:  # Phi√™n b·∫£n m·ªõi (v1)
                    # Kh·ªüi t·∫°o client theo c√°ch m·ªõi
                    client = openai.OpenAI(api_key=api_key)
                    response = client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        stream=True
                    )

                    print("‚úÖ B·∫Øt ƒë·∫ßu streaming t·ª´ OpenAI (phi√™n b·∫£n m·ªõi)...")

                    # X·ª≠ l√Ω stream phi√™n b·∫£n m·ªõi
                    for chunk in response:
                        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            yield f"data: {json.dumps({'choices': [{'delta': {'content': content}}]})}\n\n"
                else:  # Phi√™n b·∫£n c≈©
                    # Thi·∫øt l·∫≠p API key theo c√°ch c≈©
                    openai.api_key = api_key
                    # DEPRECATED: Old API no longer needed
                    # Use new API syntax instead
                    client = openai.OpenAI(api_key=api_key)
                    response = client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        stream=True
                    )

                    print("‚úÖ B·∫Øt ƒë·∫ßu streaming t·ª´ OpenAI (phi√™n b·∫£n c≈©)...")

                    # X·ª≠ l√Ω stream phi√™n b·∫£n c≈©
                    for chunk in response:
                        if "choices" in chunk and len(chunk["choices"]) > 0:
                            if "delta" in chunk["choices"][0] and "content" in chunk["choices"][0]["delta"]:
                                content = chunk["choices"][0]["delta"]["content"]
                                if content:
                                    yield f"data: {json.dumps({'choices': [{'delta': {'content': content}}]})}\n\n"

                # K·∫øt th√∫c stream
                yield "data: [DONE]\n\n"
                print("‚úÖ Streaming ho√†n t·∫•t")

            except Exception as e:
                print(f"‚ùå L·ªói streaming t·ª´ OpenAI: {str(e)}")
                error_json = json.dumps({"error": {"message": str(e)}})
                yield f"data: {error_json}\n\n"
                yield "data: [DONE]\n\n"

        # Tr·∫£ v·ªÅ streaming response
        return StreamingResponse(
            stream_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"  # NgƒÉn Nginx buffer response
            }
        )

    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω request: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": str(e)
            }
        )

# Ch·ªâ ch·∫°y khi g·ªçi tr·ª±c ti·∫øp file n√†y
if __name__ == "__main__":
    # ‚úÖ UNIFIED: Ch·∫°y tr√™n port t·ª´ environment ho·∫∑c default 8000
    port = int(os.environ.get("PORT", 8000))
    print(f"üöÄ Kh·ªüi ƒë·ªông BiBi Unified server tr√™n c·ªïng {port}...")
    uvicorn.run(app, host="0.0.0.0", port=port)
